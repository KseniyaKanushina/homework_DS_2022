{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KseniyaKanushina/homework_DS_2022/blob/main/NLP1_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Классная работа 1**"
      ],
      "metadata": {
        "id": "4ard1nWz2MyH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIvhwE4zxX0s"
      },
      "outputs": [],
      "source": [
        "#Установка нужных пакетов\n",
        "!pip install --upgrade nltk gensim bokeh umap-learn\n",
        "\n",
        "import itertools\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import umap\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# выгружаем датасет:\n",
        "!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt -nc"
      ],
      "metadata": {
        "id": "hF9WPCtfxZR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = list(open(\"./quora.txt\", encoding=\"utf-8\"))\n",
        "data[50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MaFpN9pvxtNg",
        "outputId": "9680b6a2-e9f3-4a75-d3ea-1cabb7bc9086"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"What TV shows or books help you read people's body language?\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = WordPunctTokenizer()\n",
        "\n",
        "print(tokenizer.tokenize(data[50]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvXRbOKGx0l_",
        "outputId": "fabbaa20-0df6-47d8-d1e5-de4d5a74a840"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What', 'TV', 'shows', 'or', 'books', 'help', 'you', 'read', 'people', \"'\", 's', 'body', 'language', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Задание 1: Перевести все слова в нижний регистр (NLTK) из data и добавьте как лист токенов в листе data_tok\n"
      ],
      "metadata": {
        "id": "ovkxi_QOySCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import re"
      ],
      "metadata": {
        "id": "p8lcqxFklmd7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_tok = []\n",
        "for x in data:\n",
        "    data_tok.append(tokenizer.tokenize(x.lower()))\n",
        "\n",
        "\n",
        "assert all(isinstance(row, (list, tuple)) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
        "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
        "assert all(map(lambda l: not is_latin(l) or l.islower(), map(' '.join, data_tok))), \"please make sure to lowercase the data\"\n"
      ],
      "metadata": {
        "id": "EK7uvHi6zeWY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_tok)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWjqeWI1r5UO",
        "outputId": "a11d0ecf-b604-442a-9907-582bdaafc293"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Задание 2: Подсчитайте топ10 самых популярных лем в рамках data"
      ],
      "metadata": {
        "id": "dtKeoLCYzY4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict = {}\n",
        "for lem in all_lem:\n",
        "    if lem in dict.keys():\n",
        "      dict[lem]+=1\n",
        "    else:\n",
        "      dict[lem]=1\n",
        "print(len(dict))\n",
        "print(dict)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(dict.items())\n",
        "df\n",
        "sorted_df = df.sort_values(by=1, ascending = False) \n",
        "print(sorted_df[0:10])"
      ],
      "metadata": {
        "id": "a_BxzSv9yR0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Задание 3: Подсчитайте количество разных слов до и после лемматизации"
      ],
      "metadata": {
        "id": "a1SM3sn1zf1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkOo_Ueis2-G",
        "outputId": "e8be0202-aae8-448b-f0ad-120ee7c2af63"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "for i in data_tok:\n",
        "  for j in i:\n",
        "    all_words.append(j)\n",
        "\n",
        "all_lem = [lemmatizer.lemmatize(i) for i in all_words]\n",
        "\n",
        "unique_words = set(all_words)\n",
        "print(len(unique_words),len(set(all_lem)))\n"
      ],
      "metadata": {
        "id": "Q88BIteDzpWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d26eaafa-b78f-449c-998f-f342963eb023"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87819 80303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Задание 4: Подсчитайте количество разных слов до и после стемминга"
      ],
      "metadata": {
        "id": "uxKa8yUUzqNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_st = [ps.stem(i) for i in all_words]\n",
        "\n",
        "print(len(unique_words),len(set(all_st)))\n"
      ],
      "metadata": {
        "id": "x91DX51qzszR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bb20d9c-2e80-44a6-fa50-f07536b24ab6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87819 67026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 7\n",
        "https://www.hackerrank.com/challenges/matching-whitespace-non-whitespace-character/problem?isFullScreen=true\n",
        "\n",
        "Regex_Pattern = r\"(\\w{2,2}\\s{1}){2,2}\\w{2,2}\""
      ],
      "metadata": {
        "id": "xiHmcEGfDA_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Домашнее** **задание** **1**"
      ],
      "metadata": {
        "id": "r979TQbM2Chj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Задание 5: Подсчитайте количество разных слов\n",
        "\n"
      ],
      "metadata": {
        "id": "XXA7Fe_izuqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_tok = []\n",
        "for x in data:\n",
        "    data_tok.append(tokenizer.tokenize(x.lower()))\n",
        "\n",
        "flatten_tok = []\n",
        "for x in data_tok:\n",
        "  for i in x:\n",
        "    flatten_tok.append(i)\n",
        "\n",
        "stem_lem = []\n",
        "lem_stem = []\n",
        "for x in flatten_tok:\n",
        "  stem_lem.append(lemmatizer.lemmatize(ps.stem(x)))\n",
        "  lem_stem.append(ps.stem(lemmatizer.lemmatize(x)))\n",
        "print(f'Уникальных слов: \\n Сначала лемматизацию, затем стемминг:{len(set(lem_stem))}')\n",
        "print(f' Сначала стемминг, затем лемматизацию:{len(set(stem_lem))}')"
      ],
      "metadata": {
        "id": "BGgmHzUAzwqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd404df-d588-4b41-e38b-68852fabc6c6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Уникальных слов: \n",
            " Сначала лемматизацию, затем стемминг:66835\n",
            " Сначала стемминг, затем лемматизацию:66818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML1_1:\n",
        "https://www.hackerrank.com/challenges/capturing-non-capturing-groups/problem?isFullScreen=true\n",
        "\n",
        "Regex_Pattern = r'okokok'\t"
      ],
      "metadata": {
        "id": "rsroEjZ-1wvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML1_2: https://www.hackerrank.com/challenges/branch-reset-groups/problem?isFullScreen=true\n",
        "\n",
        "\n",
        "$Regex_Pattern = '^\\d{2}(?|(-)|(---)|(:)|(.))\\d{2}(?:\\1\\d{2}){2,2}$';\n",
        "\n",
        "Использовала язык Перл"
      ],
      "metadata": {
        "id": "oxz1XQkh14CK"
      }
    }
  ]
}